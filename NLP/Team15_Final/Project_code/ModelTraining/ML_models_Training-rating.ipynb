{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e14a1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# importing plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ignoring Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54b09d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the pre processed file back \n",
    "Pre_Processed_Dataset = pd.read_csv('Pre_Processed_Dataset.csv')\n",
    "Pre_Processed_Dataset = Pre_Processed_Dataset.drop(233921)\n",
    "Pre_Processed_Dataset.reset_index(inplace = True, drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6a8d745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting histogram about lengths of reviews\n",
    "fig, ax = plt.subplots(figsize =(8, 6))\n",
    "ax.hist(Pre_Processed_Dataset['Length'], bins = [x for x in range(0,251,5)])\n",
    "plt.title(\"Histogram length of Reviews\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Count of Review\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b32f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Count of Unique Words -> 80097\n"
     ]
    }
   ],
   "source": [
    "# finding count of unique words\n",
    "Unique_Words = set()\n",
    "for Temp_Review in Pre_Processed_Dataset['Review']:\n",
    "    for Temp_Word in Temp_Review.split():\n",
    "        Unique_Words.add(Temp_Word)\n",
    "print(\"Total Count of Unique Words ->\", len(Unique_Words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98c96058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review for Rating 1 -> 52264\n",
      "Review for Rating 2 -> 29743\n",
      "Review for Rating 3 -> 42638\n",
      "Review for Rating 4 -> 80655\n",
      "Review for Rating 5 -> 363110\n",
      "So if we need to keep the data un biased the last entry i can take is 29743 X 5 (29743 from each rating)\n"
     ]
    }
   ],
   "source": [
    "# counting words\n",
    "Instance_List = list(Pre_Processed_Dataset['Rating'])\n",
    "print(\"Review for Rating 1 ->\", Instance_List.count(1))\n",
    "print(\"Review for Rating 2 ->\", Instance_List.count(2))\n",
    "print(\"Review for Rating 3 ->\", Instance_List.count(3))\n",
    "print(\"Review for Rating 4 ->\", Instance_List.count(4))\n",
    "print(\"Review for Rating 5 ->\", Instance_List.count(5))\n",
    "print(\"So if we need to keep the data un biased the last entry i can take is 29743 X 5 (29743 from each rating)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a66f9313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM,SpatialDropout1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dfd6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting bag size list\n",
    "#List_Size_Of_Bags = [1000, 2000]\n",
    "List_Size_Of_Bags = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]\n",
    "\n",
    "# data size\n",
    "# List_Data_Size = [5000, 10000]\n",
    "List_Data_Size = [5000, 10000, 15000, 20000, 25000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d96e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Bag Size -> 500\n",
      "Data size 5000 X 5 -> 25000\n",
      "Score of KNeighborsClassifier is  0.31\n",
      "Score of Logistic regression is  0.48\n",
      "Score of Random Forest is  0.51\n",
      "Score of Decision tree is  0.41\n",
      "\n",
      "Selected Bag Size -> 1000\n",
      "Data size 5000 X 5 -> 25000\n",
      "Score of KNeighborsClassifier is  0.31\n",
      "Score of Logistic regression is  0.51\n",
      "Score of Random Forest is  0.52\n",
      "Score of Decision tree is  0.42\n",
      "\n",
      "Selected Bag Size -> 1500\n",
      "Data size 5000 X 5 -> 25000\n",
      "Score of KNeighborsClassifier is  0.32\n",
      "Score of Logistic regression is  0.51\n",
      "Score of Random Forest is  0.52\n",
      "Score of Decision tree is  0.42\n",
      "\n",
      "Selected Bag Size -> 2000\n",
      "Data size 5000 X 5 -> 25000\n",
      "Score of KNeighborsClassifier is  0.32\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.51\n",
      "Score of Decision tree is  0.42\n",
      "\n",
      "Selected Bag Size -> 2500\n",
      "Data size 5000 X 5 -> 25000\n",
      "Score of KNeighborsClassifier is  0.33\n",
      "Score of Logistic regression is  0.51\n",
      "Score of Random Forest is  0.54\n",
      "Score of Decision tree is  0.41\n",
      "\n",
      "Selected Bag Size -> 3000\n",
      "Data size 5000 X 5 -> 25000\n",
      "Score of KNeighborsClassifier is  0.33\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.52\n",
      "Score of Decision tree is  0.43\n",
      "\n",
      "Selected Bag Size -> 3500\n",
      "Data size 5000 X 5 -> 25000\n",
      "Score of KNeighborsClassifier is  0.34\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.53\n",
      "Score of Decision tree is  0.42\n",
      "\n",
      "Selected Bag Size -> 4000\n",
      "Data size 5000 X 5 -> 25000\n",
      "Score of KNeighborsClassifier is  0.34\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.53\n",
      "Score of Decision tree is  0.42\n",
      "\n",
      "Selected Bag Size -> 4500\n",
      "Data size 5000 X 5 -> 25000\n",
      "Score of KNeighborsClassifier is  0.34\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.53\n",
      "Score of Decision tree is  0.42\n",
      "\n",
      "Selected Bag Size -> 5000\n",
      "Data size 5000 X 5 -> 25000\n",
      "Score of KNeighborsClassifier is  0.34\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.53\n",
      "Score of Decision tree is  0.41\n",
      "\n",
      "Selected Bag Size -> 500\n",
      "Data size 10000 X 5 -> 50000\n",
      "Score of KNeighborsClassifier is  0.35\n",
      "Score of Logistic regression is  0.48\n",
      "Score of Random Forest is  0.54\n",
      "Score of Decision tree is  0.45\n",
      "\n",
      "Selected Bag Size -> 1000\n",
      "Data size 10000 X 5 -> 50000\n",
      "Score of KNeighborsClassifier is  0.36\n",
      "Score of Logistic regression is  0.49\n",
      "Score of Random Forest is  0.55\n",
      "Score of Decision tree is  0.46\n",
      "\n",
      "Selected Bag Size -> 1500\n",
      "Data size 10000 X 5 -> 50000\n",
      "Score of KNeighborsClassifier is  0.36\n",
      "Score of Logistic regression is  0.5\n",
      "Score of Random Forest is  0.55\n",
      "Score of Decision tree is  0.45\n",
      "\n",
      "Selected Bag Size -> 2000\n",
      "Data size 10000 X 5 -> 50000\n",
      "Score of KNeighborsClassifier is  0.36\n",
      "Score of Logistic regression is  0.51\n",
      "Score of Random Forest is  0.55\n",
      "Score of Decision tree is  0.46\n",
      "\n",
      "Selected Bag Size -> 2500\n",
      "Data size 10000 X 5 -> 50000\n",
      "Score of KNeighborsClassifier is  0.36\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.56\n",
      "Score of Decision tree is  0.46\n",
      "\n",
      "Selected Bag Size -> 3000\n",
      "Data size 10000 X 5 -> 50000\n",
      "Score of KNeighborsClassifier is  0.36\n",
      "Score of Logistic regression is  0.51\n",
      "Score of Random Forest is  0.55\n",
      "Score of Decision tree is  0.46\n",
      "\n",
      "Selected Bag Size -> 3500\n",
      "Data size 10000 X 5 -> 50000\n",
      "Score of KNeighborsClassifier is  0.36\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.55\n",
      "Score of Decision tree is  0.46\n",
      "\n",
      "Selected Bag Size -> 4000\n",
      "Data size 10000 X 5 -> 50000\n",
      "Score of KNeighborsClassifier is  0.36\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.55\n",
      "Score of Decision tree is  0.46\n",
      "\n",
      "Selected Bag Size -> 4500\n",
      "Data size 10000 X 5 -> 50000\n",
      "Score of KNeighborsClassifier is  0.37\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.56\n",
      "Score of Decision tree is  0.46\n",
      "\n",
      "Selected Bag Size -> 5000\n",
      "Data size 10000 X 5 -> 50000\n",
      "Score of KNeighborsClassifier is  0.37\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.56\n",
      "Score of Decision tree is  0.45\n",
      "\n",
      "Selected Bag Size -> 500\n",
      "Data size 15000 X 5 -> 75000\n",
      "Score of KNeighborsClassifier is  0.38\n",
      "Score of Logistic regression is  0.47\n",
      "Score of Random Forest is  0.56\n",
      "Score of Decision tree is  0.47\n",
      "\n",
      "Selected Bag Size -> 1000\n",
      "Data size 15000 X 5 -> 75000\n",
      "Score of KNeighborsClassifier is  0.38\n",
      "Score of Logistic regression is  0.5\n",
      "Score of Random Forest is  0.58\n",
      "Score of Decision tree is  0.47\n",
      "\n",
      "Selected Bag Size -> 1500\n",
      "Data size 15000 X 5 -> 75000\n",
      "Score of KNeighborsClassifier is  0.39\n",
      "Score of Logistic regression is  0.51\n",
      "Score of Random Forest is  0.58\n",
      "Score of Decision tree is  0.48\n",
      "\n",
      "Selected Bag Size -> 2000\n",
      "Data size 15000 X 5 -> 75000\n",
      "Score of KNeighborsClassifier is  0.39\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.58\n",
      "Score of Decision tree is  0.48\n",
      "\n",
      "Selected Bag Size -> 2500\n",
      "Data size 15000 X 5 -> 75000\n",
      "Score of KNeighborsClassifier is  0.38\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.58\n",
      "Score of Decision tree is  0.47\n",
      "\n",
      "Selected Bag Size -> 3000\n",
      "Data size 15000 X 5 -> 75000\n",
      "Score of KNeighborsClassifier is  0.39\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.58\n",
      "Score of Decision tree is  0.47\n",
      "\n",
      "Selected Bag Size -> 3500\n",
      "Data size 15000 X 5 -> 75000\n",
      "Score of KNeighborsClassifier is  0.39\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.58\n",
      "Score of Decision tree is  0.47\n",
      "\n",
      "Selected Bag Size -> 4000\n",
      "Data size 15000 X 5 -> 75000\n",
      "Score of KNeighborsClassifier is  0.39\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.58\n",
      "Score of Decision tree is  0.47\n",
      "\n",
      "Selected Bag Size -> 4500\n",
      "Data size 15000 X 5 -> 75000\n",
      "Score of KNeighborsClassifier is  0.39\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.59\n",
      "Score of Decision tree is  0.47\n",
      "\n",
      "Selected Bag Size -> 5000\n",
      "Data size 15000 X 5 -> 75000\n",
      "Score of KNeighborsClassifier is  0.39\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.59\n",
      "Score of Decision tree is  0.48\n",
      "\n",
      "Selected Bag Size -> 500\n",
      "Data size 20000 X 5 -> 100000\n",
      "Score of KNeighborsClassifier is  0.41\n",
      "Score of Logistic regression is  0.47\n",
      "Score of Random Forest is  0.59\n",
      "Score of Decision tree is  0.51\n",
      "\n",
      "Selected Bag Size -> 1000\n",
      "Data size 20000 X 5 -> 100000\n",
      "Score of KNeighborsClassifier is  0.41\n",
      "Score of Logistic regression is  0.5\n",
      "Score of Random Forest is  0.6\n",
      "Score of Decision tree is  0.52\n",
      "\n",
      "Selected Bag Size -> 1500\n",
      "Data size 20000 X 5 -> 100000\n",
      "Score of KNeighborsClassifier is  0.41\n",
      "Score of Logistic regression is  0.51\n",
      "Score of Random Forest is  0.61\n",
      "Score of Decision tree is  0.52\n",
      "\n",
      "Selected Bag Size -> 2000\n",
      "Data size 20000 X 5 -> 100000\n",
      "Score of KNeighborsClassifier is  0.41\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.61\n",
      "Score of Decision tree is  0.52\n",
      "\n",
      "Selected Bag Size -> 2500\n",
      "Data size 20000 X 5 -> 100000\n",
      "Score of KNeighborsClassifier is  0.41\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.61\n",
      "Score of Decision tree is  0.52\n",
      "\n",
      "Selected Bag Size -> 3000\n",
      "Data size 20000 X 5 -> 100000\n",
      "Score of KNeighborsClassifier is  0.41\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.61\n",
      "Score of Decision tree is  0.52\n",
      "\n",
      "Selected Bag Size -> 3500\n",
      "Data size 20000 X 5 -> 100000\n",
      "Score of KNeighborsClassifier is  0.41\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.61\n",
      "Score of Decision tree is  0.52\n",
      "\n",
      "Selected Bag Size -> 4000\n",
      "Data size 20000 X 5 -> 100000\n",
      "Score of KNeighborsClassifier is  0.41\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.61\n",
      "Score of Decision tree is  0.52\n",
      "\n",
      "Selected Bag Size -> 4500\n",
      "Data size 20000 X 5 -> 100000\n",
      "Score of KNeighborsClassifier is  0.41\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.6\n",
      "Score of Decision tree is  0.52\n",
      "\n",
      "Selected Bag Size -> 5000\n",
      "Data size 20000 X 5 -> 100000\n",
      "Score of KNeighborsClassifier is  0.42\n",
      "Score of Logistic regression is  0.54\n",
      "Score of Random Forest is  0.61\n",
      "Score of Decision tree is  0.52\n",
      "\n",
      "Selected Bag Size -> 500\n",
      "Data size 25000 X 5 -> 125000\n",
      "Score of KNeighborsClassifier is  0.42\n",
      "Score of Logistic regression is  0.47\n",
      "Score of Random Forest is  0.6\n",
      "Score of Decision tree is  0.52\n",
      "\n",
      "Selected Bag Size -> 1000\n",
      "Data size 25000 X 5 -> 125000\n",
      "Score of KNeighborsClassifier is  0.41\n",
      "Score of Logistic regression is  0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of Random Forest is  0.61\n",
      "Score of Decision tree is  0.53\n",
      "\n",
      "Selected Bag Size -> 1500\n",
      "Data size 25000 X 5 -> 125000\n",
      "Score of KNeighborsClassifier is  0.42\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.62\n",
      "Score of Decision tree is  0.53\n",
      "\n",
      "Selected Bag Size -> 2000\n",
      "Data size 25000 X 5 -> 125000\n",
      "Score of KNeighborsClassifier is  0.42\n",
      "Score of Logistic regression is  0.52\n",
      "Score of Random Forest is  0.62\n",
      "Score of Decision tree is  0.53\n",
      "\n",
      "Selected Bag Size -> 2500\n",
      "Data size 25000 X 5 -> 125000\n",
      "Score of KNeighborsClassifier is  0.42\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.62\n",
      "Score of Decision tree is  0.53\n",
      "\n",
      "Selected Bag Size -> 3000\n",
      "Data size 25000 X 5 -> 125000\n",
      "Score of KNeighborsClassifier is  0.42\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.62\n",
      "Score of Decision tree is  0.53\n",
      "\n",
      "Selected Bag Size -> 3500\n",
      "Data size 25000 X 5 -> 125000\n",
      "Score of KNeighborsClassifier is  0.42\n",
      "Score of Logistic regression is  0.53\n",
      "Score of Random Forest is  0.62\n",
      "Score of Decision tree is  0.53\n",
      "\n",
      "Selected Bag Size -> 4000\n",
      "Data size 25000 X 5 -> 125000\n",
      "Score of KNeighborsClassifier is  0.43\n",
      "Score of Logistic regression is  0.54\n",
      "Score of Random Forest is  0.62\n",
      "Score of Decision tree is  0.53\n",
      "\n",
      "Selected Bag Size -> 4500\n",
      "Data size 25000 X 5 -> 125000\n",
      "Score of KNeighborsClassifier is  0.43\n",
      "Score of Logistic regression is  0.54\n",
      "Score of Random Forest is  0.62\n",
      "Score of Decision tree is  0.53\n",
      "\n",
      "Selected Bag Size -> 5000\n",
      "Data size 25000 X 5 -> 125000\n",
      "Score of KNeighborsClassifier is  0.42\n",
      "Score of Logistic regression is  0.54\n",
      "Score of Random Forest is  0.62\n",
      "Score of Decision tree is  0.54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# list to save accuracies\n",
    "Lis_of_Accuracies = []\n",
    "\n",
    "# takign size of data \n",
    "for Data_Size in List_Data_Size:\n",
    "\n",
    "    # lopping all bag size\n",
    "    for Bag_Size in List_Size_Of_Bags:\n",
    "\n",
    "        # print bag size\n",
    "        print(\"Selected Bag Size ->\", Bag_Size)\n",
    "        print(\"Data size \",Data_Size,\" X 5 -> \", Data_Size*5, sep=\"\")\n",
    "        \n",
    "        # creating segments\n",
    "        Data_Segment_1 = Pre_Processed_Dataset[Pre_Processed_Dataset['Rating'] == 1].iloc[:Data_Size,:]\n",
    "        Data_Segment_2 = Pre_Processed_Dataset[Pre_Processed_Dataset['Rating'] == 2].iloc[:Data_Size,:]\n",
    "        Data_Segment_3 = Pre_Processed_Dataset[Pre_Processed_Dataset['Rating'] == 3].iloc[:Data_Size,:]\n",
    "        Data_Segment_4 = Pre_Processed_Dataset[Pre_Processed_Dataset['Rating'] == 4].iloc[:Data_Size,:]\n",
    "        Data_Segment_5 = Pre_Processed_Dataset[Pre_Processed_Dataset['Rating'] == 5].iloc[:Data_Size,:]\n",
    "        \n",
    "        # makign new dataframe\n",
    "        New_Instance_dataframe = frames = pd.concat([Data_Segment_1, Data_Segment_2, Data_Segment_3, Data_Segment_4, Data_Segment_5])\n",
    "        New_Instance_dataframe.reset_index(inplace = True, drop = True)\n",
    "        \n",
    "        # setting features \n",
    "        Features_Collection = New_Instance_dataframe['Review']\n",
    "\n",
    "        # setting labels \n",
    "        Label_Collection = New_Instance_dataframe['Rating']\n",
    "        \n",
    "        # making the word bag\n",
    "        cv = CountVectorizer(max_features = Bag_Size)\n",
    "        Instance_Features = cv.fit_transform(Features_Collection).toarray()\n",
    "        Instance_Features = pd.DataFrame(Instance_Features)\n",
    "        \n",
    "        # Splitting the dataset into the Training set and Test set\n",
    "        features_train, features_test, labels_train, labels_test = train_test_split(Instance_Features, Label_Collection, test_size = 0.10, random_state = 123)\n",
    "        \n",
    "        # kneighbor classifer\n",
    "        classifier = KNeighborsClassifier()\n",
    "        classifier.fit(features_train, labels_train)\n",
    "        Score_1 = classifier.score(features_test,labels_test)\n",
    "        print(\"Score of KNeighborsClassifier is \",round(Score_1,2))\n",
    "        \n",
    "        # logistic regression\n",
    "        classifier = LogisticRegression()\n",
    "        classifier.fit(features_train, labels_train)\n",
    "        Score_2 = classifier.score(features_test,labels_test)\n",
    "        print(\"Score of Logistic regression is \",round(Score_2,2))\n",
    "        \n",
    "        classifier = RandomForestClassifier()\n",
    "        classifier.fit(features_train, labels_train)  \n",
    "        Score_3 = classifier.score(features_test,labels_test)\n",
    "        print(\"Score of Random Forest is \",round(Score_3,2))\n",
    "        \n",
    "        # decision tree classifier\n",
    "        classifier = DecisionTreeClassifier() \n",
    "        classifier.fit(features_train, labels_train) \n",
    "        Score_4 = classifier.score(features_test,labels_test)\n",
    "        print(\"Score of Decision tree is \",round(Score_4,2))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        # end\n",
    "        Lis_of_Accuracies.append([Data_Size, Bag_Size, Score_1, Score_2, Score_3, Score_4])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2fef71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the results into datafrem\n",
    "Results_Dataset = pd.DataFrame(Lis_of_Accuracies)\n",
    "\n",
    "# setting the column names of the \n",
    "Results_Dataset.columns = ['Data Size', 'Bag Size', 'KNC', 'LR', 'RFC', 'DTC']\n",
    "\n",
    "# dumping the results into the file \n",
    "Results_Dataset.to_csv('Results Data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17395ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Size</th>\n",
       "      <th>Bag Size</th>\n",
       "      <th>KNC</th>\n",
       "      <th>LR</th>\n",
       "      <th>RFC</th>\n",
       "      <th>DTC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>500</td>\n",
       "      <td>0.310800</td>\n",
       "      <td>0.483200</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.413600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.312400</td>\n",
       "      <td>0.512400</td>\n",
       "      <td>0.523200</td>\n",
       "      <td>0.418800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5000</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.323200</td>\n",
       "      <td>0.514000</td>\n",
       "      <td>0.520400</td>\n",
       "      <td>0.419600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.324800</td>\n",
       "      <td>0.516400</td>\n",
       "      <td>0.513600</td>\n",
       "      <td>0.418800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5000</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.514400</td>\n",
       "      <td>0.536800</td>\n",
       "      <td>0.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.523600</td>\n",
       "      <td>0.524000</td>\n",
       "      <td>0.426000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5000</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.338800</td>\n",
       "      <td>0.518400</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5000</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.338400</td>\n",
       "      <td>0.518400</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5000</td>\n",
       "      <td>4500</td>\n",
       "      <td>0.338000</td>\n",
       "      <td>0.519200</td>\n",
       "      <td>0.529600</td>\n",
       "      <td>0.420400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.522400</td>\n",
       "      <td>0.528400</td>\n",
       "      <td>0.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10000</td>\n",
       "      <td>500</td>\n",
       "      <td>0.351400</td>\n",
       "      <td>0.482000</td>\n",
       "      <td>0.536400</td>\n",
       "      <td>0.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.356400</td>\n",
       "      <td>0.494400</td>\n",
       "      <td>0.548800</td>\n",
       "      <td>0.460400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10000</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.356000</td>\n",
       "      <td>0.499000</td>\n",
       "      <td>0.547000</td>\n",
       "      <td>0.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.359600</td>\n",
       "      <td>0.508400</td>\n",
       "      <td>0.554600</td>\n",
       "      <td>0.457600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10000</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.355200</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.559800</td>\n",
       "      <td>0.461000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.362600</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.554400</td>\n",
       "      <td>0.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10000</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.361000</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>0.553600</td>\n",
       "      <td>0.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10000</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.365000</td>\n",
       "      <td>0.518400</td>\n",
       "      <td>0.550600</td>\n",
       "      <td>0.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10000</td>\n",
       "      <td>4500</td>\n",
       "      <td>0.367800</td>\n",
       "      <td>0.521600</td>\n",
       "      <td>0.557200</td>\n",
       "      <td>0.455400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.370400</td>\n",
       "      <td>0.522000</td>\n",
       "      <td>0.561200</td>\n",
       "      <td>0.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15000</td>\n",
       "      <td>500</td>\n",
       "      <td>0.378533</td>\n",
       "      <td>0.470533</td>\n",
       "      <td>0.561333</td>\n",
       "      <td>0.466133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>15000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.381333</td>\n",
       "      <td>0.500400</td>\n",
       "      <td>0.577867</td>\n",
       "      <td>0.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15000</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.386400</td>\n",
       "      <td>0.508533</td>\n",
       "      <td>0.581867</td>\n",
       "      <td>0.483733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>15000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.389600</td>\n",
       "      <td>0.515333</td>\n",
       "      <td>0.581333</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15000</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.384667</td>\n",
       "      <td>0.522267</td>\n",
       "      <td>0.580933</td>\n",
       "      <td>0.474933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>15000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.386800</td>\n",
       "      <td>0.526133</td>\n",
       "      <td>0.578267</td>\n",
       "      <td>0.474133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>15000</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>0.529200</td>\n",
       "      <td>0.578000</td>\n",
       "      <td>0.474667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15000</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.389333</td>\n",
       "      <td>0.531733</td>\n",
       "      <td>0.581600</td>\n",
       "      <td>0.470667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>15000</td>\n",
       "      <td>4500</td>\n",
       "      <td>0.391333</td>\n",
       "      <td>0.531067</td>\n",
       "      <td>0.590667</td>\n",
       "      <td>0.473600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.390533</td>\n",
       "      <td>0.529333</td>\n",
       "      <td>0.586267</td>\n",
       "      <td>0.476000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>20000</td>\n",
       "      <td>500</td>\n",
       "      <td>0.414800</td>\n",
       "      <td>0.474600</td>\n",
       "      <td>0.593000</td>\n",
       "      <td>0.510300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>20000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.407900</td>\n",
       "      <td>0.500900</td>\n",
       "      <td>0.603600</td>\n",
       "      <td>0.516300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>20000</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.411700</td>\n",
       "      <td>0.511700</td>\n",
       "      <td>0.611400</td>\n",
       "      <td>0.515300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>20000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.411900</td>\n",
       "      <td>0.518100</td>\n",
       "      <td>0.609200</td>\n",
       "      <td>0.518900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>20000</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.406900</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>0.607800</td>\n",
       "      <td>0.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>20000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.407500</td>\n",
       "      <td>0.528700</td>\n",
       "      <td>0.607100</td>\n",
       "      <td>0.516900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>20000</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.412500</td>\n",
       "      <td>0.531300</td>\n",
       "      <td>0.608400</td>\n",
       "      <td>0.521400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>20000</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>0.534300</td>\n",
       "      <td>0.608700</td>\n",
       "      <td>0.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>20000</td>\n",
       "      <td>4500</td>\n",
       "      <td>0.414700</td>\n",
       "      <td>0.534900</td>\n",
       "      <td>0.604300</td>\n",
       "      <td>0.523900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>20000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.415000</td>\n",
       "      <td>0.536200</td>\n",
       "      <td>0.611900</td>\n",
       "      <td>0.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>25000</td>\n",
       "      <td>500</td>\n",
       "      <td>0.420320</td>\n",
       "      <td>0.472800</td>\n",
       "      <td>0.601680</td>\n",
       "      <td>0.521440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>25000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.414640</td>\n",
       "      <td>0.498560</td>\n",
       "      <td>0.613280</td>\n",
       "      <td>0.530960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25000</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.423760</td>\n",
       "      <td>0.516240</td>\n",
       "      <td>0.618240</td>\n",
       "      <td>0.528880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>25000</td>\n",
       "      <td>2000</td>\n",
       "      <td>0.419280</td>\n",
       "      <td>0.522320</td>\n",
       "      <td>0.617200</td>\n",
       "      <td>0.529040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>25000</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.417360</td>\n",
       "      <td>0.528480</td>\n",
       "      <td>0.620560</td>\n",
       "      <td>0.530320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>25000</td>\n",
       "      <td>3000</td>\n",
       "      <td>0.417200</td>\n",
       "      <td>0.530400</td>\n",
       "      <td>0.622560</td>\n",
       "      <td>0.531760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>25000</td>\n",
       "      <td>3500</td>\n",
       "      <td>0.420960</td>\n",
       "      <td>0.532880</td>\n",
       "      <td>0.620800</td>\n",
       "      <td>0.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>25000</td>\n",
       "      <td>4000</td>\n",
       "      <td>0.426400</td>\n",
       "      <td>0.536160</td>\n",
       "      <td>0.623040</td>\n",
       "      <td>0.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>25000</td>\n",
       "      <td>4500</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.537920</td>\n",
       "      <td>0.617360</td>\n",
       "      <td>0.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>25000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.424240</td>\n",
       "      <td>0.541520</td>\n",
       "      <td>0.620880</td>\n",
       "      <td>0.536000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Data Size  Bag Size       KNC        LR       RFC       DTC\n",
       "0        5000       500  0.310800  0.483200  0.510000  0.413600\n",
       "1        5000      1000  0.312400  0.512400  0.523200  0.418800\n",
       "2        5000      1500  0.323200  0.514000  0.520400  0.419600\n",
       "3        5000      2000  0.324800  0.516400  0.513600  0.418800\n",
       "4        5000      2500  0.329200  0.514400  0.536800  0.413200\n",
       "5        5000      3000  0.329200  0.523600  0.524000  0.426000\n",
       "6        5000      3500  0.338800  0.518400  0.530000  0.418000\n",
       "7        5000      4000  0.338400  0.518400  0.534000  0.420400\n",
       "8        5000      4500  0.338000  0.519200  0.529600  0.420400\n",
       "9        5000      5000  0.340000  0.522400  0.528400  0.413200\n",
       "10      10000       500  0.351400  0.482000  0.536400  0.451400\n",
       "11      10000      1000  0.356400  0.494400  0.548800  0.460400\n",
       "12      10000      1500  0.356000  0.499000  0.547000  0.445800\n",
       "13      10000      2000  0.359600  0.508400  0.554600  0.457600\n",
       "14      10000      2500  0.355200  0.515000  0.559800  0.461000\n",
       "15      10000      3000  0.362600  0.512600  0.554400  0.455800\n",
       "16      10000      3500  0.361000  0.516000  0.553600  0.455400\n",
       "17      10000      4000  0.365000  0.518400  0.550600  0.459000\n",
       "18      10000      4500  0.367800  0.521600  0.557200  0.455400\n",
       "19      10000      5000  0.370400  0.522000  0.561200  0.452200\n",
       "20      15000       500  0.378533  0.470533  0.561333  0.466133\n",
       "21      15000      1000  0.381333  0.500400  0.577867  0.468000\n",
       "22      15000      1500  0.386400  0.508533  0.581867  0.483733\n",
       "23      15000      2000  0.389600  0.515333  0.581333  0.476800\n",
       "24      15000      2500  0.384667  0.522267  0.580933  0.474933\n",
       "25      15000      3000  0.386800  0.526133  0.578267  0.474133\n",
       "26      15000      3500  0.386667  0.529200  0.578000  0.474667\n",
       "27      15000      4000  0.389333  0.531733  0.581600  0.470667\n",
       "28      15000      4500  0.391333  0.531067  0.590667  0.473600\n",
       "29      15000      5000  0.390533  0.529333  0.586267  0.476000\n",
       "30      20000       500  0.414800  0.474600  0.593000  0.510300\n",
       "31      20000      1000  0.407900  0.500900  0.603600  0.516300\n",
       "32      20000      1500  0.411700  0.511700  0.611400  0.515300\n",
       "33      20000      2000  0.411900  0.518100  0.609200  0.518900\n",
       "34      20000      2500  0.406900  0.524500  0.607800  0.521300\n",
       "35      20000      3000  0.407500  0.528700  0.607100  0.516900\n",
       "36      20000      3500  0.412500  0.531300  0.608400  0.521400\n",
       "37      20000      4000  0.411400  0.534300  0.608700  0.521800\n",
       "38      20000      4500  0.414700  0.534900  0.604300  0.523900\n",
       "39      20000      5000  0.415000  0.536200  0.611900  0.523100\n",
       "40      25000       500  0.420320  0.472800  0.601680  0.521440\n",
       "41      25000      1000  0.414640  0.498560  0.613280  0.530960\n",
       "42      25000      1500  0.423760  0.516240  0.618240  0.528880\n",
       "43      25000      2000  0.419280  0.522320  0.617200  0.529040\n",
       "44      25000      2500  0.417360  0.528480  0.620560  0.530320\n",
       "45      25000      3000  0.417200  0.530400  0.622560  0.531760\n",
       "46      25000      3500  0.420960  0.532880  0.620800  0.532800\n",
       "47      25000      4000  0.426400  0.536160  0.623040  0.530000\n",
       "48      25000      4500  0.426800  0.537920  0.617360  0.531600\n",
       "49      25000      5000  0.424240  0.541520  0.620880  0.536000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('Results Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa6c500e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best accuracy is achived on datasize 25000 and bag size 4000, i.e  KNeighborsClassifier 42.6400% , Logistic regression 53.61% , Random Forest 62.30% ,Decision tree 53.00%\n"
     ]
    }
   ],
   "source": [
    "print(\"The best accuracy is achived on datasize 25000 and bag size 4000, i.e  KNeighborsClassifier 42.6400% , Logistic regression 53.61% , Random Forest 62.30% ,Decision tree 53.00%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f231bcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
